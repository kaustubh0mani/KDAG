{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.loc[data[\"class\"] == \"n\",\"class\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.loc[data[\"class\"] == \"w\",\"class\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[[\"class\",\"GLCM_pan\",\"Mean_Green\",\"Mean_Red\",\"Mean_NIR\",\"SD_pan\"]] = data[[\"class\",\"GLCM_pan\",\"Mean_Green\",\"Mean_Red\",\"Mean_NIR\",\"SD_pan\"]].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Splitting Data into Training and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "data_train,data_cv = train_test_split(data,test_size = 0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GLCM_pan</th>\n",
       "      <th>Mean_Green</th>\n",
       "      <th>Mean_Red</th>\n",
       "      <th>Mean_NIR</th>\n",
       "      <th>SD_pan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>130.444279</td>\n",
       "      <td>197.188976</td>\n",
       "      <td>89.362205</td>\n",
       "      <td>201.582677</td>\n",
       "      <td>16.734460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>130.956113</td>\n",
       "      <td>197.300000</td>\n",
       "      <td>82.850000</td>\n",
       "      <td>472.550000</td>\n",
       "      <td>17.039586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>109.176136</td>\n",
       "      <td>248.181818</td>\n",
       "      <td>125.545454</td>\n",
       "      <td>708.545455</td>\n",
       "      <td>16.789903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>107.362500</td>\n",
       "      <td>210.200000</td>\n",
       "      <td>95.400000</td>\n",
       "      <td>512.200000</td>\n",
       "      <td>26.452977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3825</th>\n",
       "      <td>129.015665</td>\n",
       "      <td>236.551839</td>\n",
       "      <td>130.869565</td>\n",
       "      <td>375.852843</td>\n",
       "      <td>29.515514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3704</th>\n",
       "      <td>123.472630</td>\n",
       "      <td>278.234043</td>\n",
       "      <td>134.021277</td>\n",
       "      <td>736.446808</td>\n",
       "      <td>20.666881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>146.487500</td>\n",
       "      <td>244.100000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>760.100000</td>\n",
       "      <td>27.946377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>165.725000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>608.000000</td>\n",
       "      <td>11.285389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>120.539246</td>\n",
       "      <td>236.580645</td>\n",
       "      <td>127.645161</td>\n",
       "      <td>606.870968</td>\n",
       "      <td>26.660451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>138.614314</td>\n",
       "      <td>213.603175</td>\n",
       "      <td>126.111111</td>\n",
       "      <td>421.000000</td>\n",
       "      <td>18.304925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>138.724138</td>\n",
       "      <td>215.727273</td>\n",
       "      <td>99.727273</td>\n",
       "      <td>473.272727</td>\n",
       "      <td>20.349021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3971</th>\n",
       "      <td>117.725705</td>\n",
       "      <td>404.300000</td>\n",
       "      <td>282.800000</td>\n",
       "      <td>451.750000</td>\n",
       "      <td>34.156981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3621</th>\n",
       "      <td>116.423591</td>\n",
       "      <td>203.917647</td>\n",
       "      <td>88.682353</td>\n",
       "      <td>531.447059</td>\n",
       "      <td>31.058053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>130.591709</td>\n",
       "      <td>192.780000</td>\n",
       "      <td>85.400000</td>\n",
       "      <td>540.500000</td>\n",
       "      <td>24.079444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>129.824580</td>\n",
       "      <td>206.350000</td>\n",
       "      <td>88.816667</td>\n",
       "      <td>325.266667</td>\n",
       "      <td>20.614315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>134.028807</td>\n",
       "      <td>235.327869</td>\n",
       "      <td>109.786885</td>\n",
       "      <td>720.295082</td>\n",
       "      <td>30.939460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2490</th>\n",
       "      <td>132.696225</td>\n",
       "      <td>322.420690</td>\n",
       "      <td>202.655172</td>\n",
       "      <td>271.634483</td>\n",
       "      <td>9.695065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2651</th>\n",
       "      <td>144.229167</td>\n",
       "      <td>246.666667</td>\n",
       "      <td>116.666667</td>\n",
       "      <td>823.666667</td>\n",
       "      <td>33.005050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>140.543750</td>\n",
       "      <td>225.950000</td>\n",
       "      <td>101.900000</td>\n",
       "      <td>724.300000</td>\n",
       "      <td>19.578751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>125.363763</td>\n",
       "      <td>227.881482</td>\n",
       "      <td>104.229630</td>\n",
       "      <td>643.140741</td>\n",
       "      <td>28.009830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>119.750417</td>\n",
       "      <td>234.039735</td>\n",
       "      <td>108.238411</td>\n",
       "      <td>687.847682</td>\n",
       "      <td>34.121691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3213</th>\n",
       "      <td>119.319318</td>\n",
       "      <td>208.145631</td>\n",
       "      <td>90.233010</td>\n",
       "      <td>476.796116</td>\n",
       "      <td>32.798516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>145.081028</td>\n",
       "      <td>208.281250</td>\n",
       "      <td>108.250000</td>\n",
       "      <td>269.906250</td>\n",
       "      <td>20.069581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3785</th>\n",
       "      <td>122.452321</td>\n",
       "      <td>226.034608</td>\n",
       "      <td>110.783242</td>\n",
       "      <td>506.950820</td>\n",
       "      <td>33.628114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3509</th>\n",
       "      <td>122.150877</td>\n",
       "      <td>307.722222</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>36.228058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3188</th>\n",
       "      <td>127.437819</td>\n",
       "      <td>256.567568</td>\n",
       "      <td>129.378378</td>\n",
       "      <td>633.351351</td>\n",
       "      <td>15.997717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2361</th>\n",
       "      <td>109.369347</td>\n",
       "      <td>215.400000</td>\n",
       "      <td>95.960000</td>\n",
       "      <td>637.280000</td>\n",
       "      <td>33.618661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>102.613527</td>\n",
       "      <td>242.230769</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>634.307692</td>\n",
       "      <td>14.804045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>138.956250</td>\n",
       "      <td>209.600000</td>\n",
       "      <td>96.200000</td>\n",
       "      <td>508.400000</td>\n",
       "      <td>11.685889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063</th>\n",
       "      <td>117.470370</td>\n",
       "      <td>227.058823</td>\n",
       "      <td>111.823529</td>\n",
       "      <td>467.764706</td>\n",
       "      <td>22.642398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3889</th>\n",
       "      <td>130.410363</td>\n",
       "      <td>217.834711</td>\n",
       "      <td>97.570248</td>\n",
       "      <td>580.512397</td>\n",
       "      <td>33.732814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2445</th>\n",
       "      <td>131.930649</td>\n",
       "      <td>267.785714</td>\n",
       "      <td>138.035714</td>\n",
       "      <td>424.857143</td>\n",
       "      <td>15.474592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>183.281250</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>616.000000</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4046</th>\n",
       "      <td>128.689394</td>\n",
       "      <td>492.520000</td>\n",
       "      <td>271.120000</td>\n",
       "      <td>405.040000</td>\n",
       "      <td>18.139636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>138.233823</td>\n",
       "      <td>214.418605</td>\n",
       "      <td>97.441860</td>\n",
       "      <td>730.813954</td>\n",
       "      <td>26.142742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>137.495599</td>\n",
       "      <td>205.098591</td>\n",
       "      <td>88.732394</td>\n",
       "      <td>527.295775</td>\n",
       "      <td>27.274465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2049</th>\n",
       "      <td>125.763988</td>\n",
       "      <td>241.840000</td>\n",
       "      <td>127.617143</td>\n",
       "      <td>435.308571</td>\n",
       "      <td>28.139509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>145.056250</td>\n",
       "      <td>252.800000</td>\n",
       "      <td>115.800000</td>\n",
       "      <td>801.400000</td>\n",
       "      <td>14.548196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2625</th>\n",
       "      <td>133.862559</td>\n",
       "      <td>229.849057</td>\n",
       "      <td>99.075472</td>\n",
       "      <td>739.358491</td>\n",
       "      <td>31.478405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>115.953125</td>\n",
       "      <td>374.000000</td>\n",
       "      <td>264.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>49.585154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>116.011111</td>\n",
       "      <td>234.705882</td>\n",
       "      <td>106.352941</td>\n",
       "      <td>788.117647</td>\n",
       "      <td>20.013144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>115.206536</td>\n",
       "      <td>251.729167</td>\n",
       "      <td>120.500000</td>\n",
       "      <td>763.166667</td>\n",
       "      <td>15.326935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4292</th>\n",
       "      <td>116.829550</td>\n",
       "      <td>204.765957</td>\n",
       "      <td>89.531915</td>\n",
       "      <td>469.914894</td>\n",
       "      <td>40.278746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3709</th>\n",
       "      <td>128.549606</td>\n",
       "      <td>187.233177</td>\n",
       "      <td>83.622848</td>\n",
       "      <td>152.564945</td>\n",
       "      <td>10.311843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3111</th>\n",
       "      <td>125.129400</td>\n",
       "      <td>260.961538</td>\n",
       "      <td>123.599359</td>\n",
       "      <td>742.875000</td>\n",
       "      <td>11.465201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886</th>\n",
       "      <td>125.042832</td>\n",
       "      <td>214.750000</td>\n",
       "      <td>95.305556</td>\n",
       "      <td>444.902778</td>\n",
       "      <td>23.274591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>126.956098</td>\n",
       "      <td>240.262136</td>\n",
       "      <td>113.495146</td>\n",
       "      <td>644.320388</td>\n",
       "      <td>21.356174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>125.490628</td>\n",
       "      <td>255.315789</td>\n",
       "      <td>118.701754</td>\n",
       "      <td>814.719298</td>\n",
       "      <td>21.553644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>130.979167</td>\n",
       "      <td>221.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>19.815819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>140.581722</td>\n",
       "      <td>190.138889</td>\n",
       "      <td>84.694444</td>\n",
       "      <td>397.277778</td>\n",
       "      <td>21.896149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3755</th>\n",
       "      <td>134.119789</td>\n",
       "      <td>217.419708</td>\n",
       "      <td>98.602190</td>\n",
       "      <td>546.416058</td>\n",
       "      <td>30.872029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971</th>\n",
       "      <td>121.773307</td>\n",
       "      <td>269.937500</td>\n",
       "      <td>140.968750</td>\n",
       "      <td>547.218750</td>\n",
       "      <td>36.654945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3908</th>\n",
       "      <td>129.906030</td>\n",
       "      <td>215.437500</td>\n",
       "      <td>98.412500</td>\n",
       "      <td>570.875000</td>\n",
       "      <td>33.534774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>125.163793</td>\n",
       "      <td>266.034483</td>\n",
       "      <td>159.827586</td>\n",
       "      <td>266.896552</td>\n",
       "      <td>29.105736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>94.601562</td>\n",
       "      <td>282.125000</td>\n",
       "      <td>167.375000</td>\n",
       "      <td>496.750000</td>\n",
       "      <td>9.399967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359</th>\n",
       "      <td>132.277946</td>\n",
       "      <td>204.714286</td>\n",
       "      <td>88.666667</td>\n",
       "      <td>344.952381</td>\n",
       "      <td>18.466124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3025</th>\n",
       "      <td>120.056047</td>\n",
       "      <td>237.348837</td>\n",
       "      <td>118.930233</td>\n",
       "      <td>613.023256</td>\n",
       "      <td>22.671592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>133.871359</td>\n",
       "      <td>211.192308</td>\n",
       "      <td>94.461538</td>\n",
       "      <td>555.115385</td>\n",
       "      <td>24.896026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2938</th>\n",
       "      <td>142.054688</td>\n",
       "      <td>204.750000</td>\n",
       "      <td>90.625000</td>\n",
       "      <td>375.125000</td>\n",
       "      <td>18.261875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>122.847826</td>\n",
       "      <td>352.786885</td>\n",
       "      <td>265.934426</td>\n",
       "      <td>380.098361</td>\n",
       "      <td>18.186528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>868 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        GLCM_pan  Mean_Green    Mean_Red    Mean_NIR     SD_pan\n",
       "2089  130.444279  197.188976   89.362205  201.582677  16.734460\n",
       "1077  130.956113  197.300000   82.850000  472.550000  17.039586\n",
       "1495  109.176136  248.181818  125.545454  708.545455  16.789903\n",
       "555   107.362500  210.200000   95.400000  512.200000  26.452977\n",
       "3825  129.015665  236.551839  130.869565  375.852843  29.515514\n",
       "3704  123.472630  278.234043  134.021277  736.446808  20.666881\n",
       "545   146.487500  244.100000  110.000000  760.100000  27.946377\n",
       "675   165.725000  222.000000  106.000000  608.000000  11.285389\n",
       "4008  120.539246  236.580645  127.645161  606.870968  26.660451\n",
       "72    138.614314  213.603175  126.111111  421.000000  18.304925\n",
       "1116  138.724138  215.727273   99.727273  473.272727  20.349021\n",
       "3971  117.725705  404.300000  282.800000  451.750000  34.156981\n",
       "3621  116.423591  203.917647   88.682353  531.447059  31.058053\n",
       "142   130.591709  192.780000   85.400000  540.500000  24.079444\n",
       "2070  129.824580  206.350000   88.816667  325.266667  20.614315\n",
       "3377  134.028807  235.327869  109.786885  720.295082  30.939460\n",
       "2490  132.696225  322.420690  202.655172  271.634483   9.695065\n",
       "2651  144.229167  246.666667  116.666667  823.666667  33.005050\n",
       "1501  140.543750  225.950000  101.900000  724.300000  19.578751\n",
       "1831  125.363763  227.881482  104.229630  643.140741  28.009830\n",
       "1943  119.750417  234.039735  108.238411  687.847682  34.121691\n",
       "3213  119.319318  208.145631   90.233010  476.796116  32.798516\n",
       "776   145.081028  208.281250  108.250000  269.906250  20.069581\n",
       "3785  122.452321  226.034608  110.783242  506.950820  33.628114\n",
       "3509  122.150877  307.722222  200.000000  360.000000  36.228058\n",
       "3188  127.437819  256.567568  129.378378  633.351351  15.997717\n",
       "2361  109.369347  215.400000   95.960000  637.280000  33.618661\n",
       "794   102.613527  242.230769  123.000000  634.307692  14.804045\n",
       "379   138.956250  209.600000   96.200000  508.400000  11.685889\n",
       "3063  117.470370  227.058823  111.823529  467.764706  22.642398\n",
       "...          ...         ...         ...         ...        ...\n",
       "3889  130.410363  217.834711   97.570248  580.512397  33.732814\n",
       "2445  131.930649  267.785714  138.035714  424.857143  15.474592\n",
       "1240  183.281250  233.000000  108.000000  616.000000   9.500000\n",
       "4046  128.689394  492.520000  271.120000  405.040000  18.139636\n",
       "118   138.233823  214.418605   97.441860  730.813954  26.142742\n",
       "1669  137.495599  205.098591   88.732394  527.295775  27.274465\n",
       "2049  125.763988  241.840000  127.617143  435.308571  28.139509\n",
       "598   145.056250  252.800000  115.800000  801.400000  14.548196\n",
       "2625  133.862559  229.849057   99.075472  739.358491  31.478405\n",
       "882   115.953125  374.000000  264.000000  492.000000  49.585154\n",
       "615   116.011111  234.705882  106.352941  788.117647  20.013144\n",
       "227   115.206536  251.729167  120.500000  763.166667  15.326935\n",
       "4292  116.829550  204.765957   89.531915  469.914894  40.278746\n",
       "3709  128.549606  187.233177   83.622848  152.564945  10.311843\n",
       "3111  125.129400  260.961538  123.599359  742.875000  11.465201\n",
       "2886  125.042832  214.750000   95.305556  444.902778  23.274591\n",
       "1419  126.956098  240.262136  113.495146  644.320388  21.356174\n",
       "322   125.490628  255.315789  118.701754  814.719298  21.553644\n",
       "1327  130.979167  221.000000  100.000000  601.000000  19.815819\n",
       "1593  140.581722  190.138889   84.694444  397.277778  21.896149\n",
       "3755  134.119789  217.419708   98.602190  546.416058  30.872029\n",
       "2971  121.773307  269.937500  140.968750  547.218750  36.654945\n",
       "3908  129.906030  215.437500   98.412500  570.875000  33.534774\n",
       "971   125.163793  266.034483  159.827586  266.896552  29.105736\n",
       "836    94.601562  282.125000  167.375000  496.750000   9.399967\n",
       "1359  132.277946  204.714286   88.666667  344.952381  18.466124\n",
       "3025  120.056047  237.348837  118.930233  613.023256  22.671592\n",
       "1818  133.871359  211.192308   94.461538  555.115385  24.896026\n",
       "2938  142.054688  204.750000   90.625000  375.125000  18.261875\n",
       "917   122.847826  352.786885  265.934426  380.098361  18.186528\n",
       "\n",
       "[868 rows x 5 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_train = data_train[\"class\"]\n",
    "\n",
    "data_train = data_train.drop(\"class\",axis = 1)\n",
    "\n",
    "label_cv = data_cv[\"class\"]\n",
    "\n",
    "data_cv = data_cv.drop(\"class\",axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Exploratory part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.PairGrid at 0x11e048e90>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.plt.show()\n",
    "\n",
    "\n",
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"Mean_Red\",y=\"class\",data=data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(data_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict = clf.predict(data_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_cv = np.array(label_cv)\n",
    "\n",
    "predict = np.array(predict)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "\n",
    "accuracy_glm = accuracy_score(label_cv,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.978110599078341"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_glm\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(label_cv,predict)\n",
    "\n",
    "accuracy = float(848+1)/(848+4+15+1)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaustubh/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:931: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.095238095238095233"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(label_cv,predict,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train1 = data_train.drop(\"Mean_Red\",axis =1)\n",
    "\n",
    "data_cv1 = data_cv.drop(\"Mean_Red\",axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.fit(data_train1,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict1 = clf1.predict(data_cv1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaustubh/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(label_cv,predict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf2 = RandomForestClassifier(n_estimators=5,min_samples_split=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=5,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.fit(data_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9896313364055299"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = clf2.predict(data_cv)\n",
    "\n",
    "confusion_matrix(label_cv,predict)\n",
    "\n",
    "float(851+8)/(851+1+8+8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64000000000000001"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(label_cv,predict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"testing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'w',\n",
       " 'w',\n",
       " 'w',\n",
       " 'n',\n",
       " 'w',\n",
       " 'w',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'w',\n",
       " 'w',\n",
       " 'w',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'w',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'w',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'w',\n",
       " 'w',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'w',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'w',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'w',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'w',\n",
       " 'w',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'w',\n",
       " 'w',\n",
       " 'w',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'w',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'w',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n',\n",
       " 'n']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict2 = clf2.predict(test)\n",
    "\n",
    "predict2\n",
    "predict2 = [\"n\" if i==0. else \"w\" for i in predict2]\n",
    "\n",
    "predict2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test[\"pred_class\"] = predict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf3 = svm.SVC()\n",
    "\n",
    "clf3.fit(data_train,label_train)\n",
    "\n",
    "predict3 = clf3.predict(data_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "test.to_excel('test_result.xlsx',encoding = 'utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
